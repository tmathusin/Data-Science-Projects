{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "Treating imbalance class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:1.3.4\n",
      "numpy version:1.20.3\n",
      "h2o version:3.36.0.4\n",
      "sklearn version:1.1.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reloading\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas version:{}'.format(pd.__version__))\n",
    "import numpy as np\n",
    "print('numpy version:{}'.format(np.__version__))\n",
    "import math\n",
    "from datetime import datetime\n",
    "from dgpylib import dg_athena\n",
    "from dgpylib import dg_s3\n",
    "import os\n",
    "import re\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../Modelling')\n",
    "import helper_function2 as hf\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../EDA')\n",
    "import data_integrity_fixer as dint\n",
    "import basicstatsandplotter as bsp\n",
    "import data_imputer as di\n",
    "import data_integrity_fixer as dif\n",
    "import parameters\n",
    "\n",
    "##for modelling\n",
    "import h2o\n",
    "print('h2o version:{}'.format(h2o.__version__))\n",
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators import H2OTargetEncoderEstimator\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "from h2o.tree import H2OTree\n",
    "from h2o.tree import H2ONode\n",
    "from h2o.tree import H2OSplitNode\n",
    "from h2o.tree import H2OLeafNode\n",
    "from h2o.estimators.kmeans import H2OKMeansEstimator\n",
    "\n",
    "#import xgboost as xgb\n",
    "import sklearn\n",
    "import category_encoders as ce\n",
    "print('sklearn version:{}'.format(sklearn.__version__))\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import time\n",
    "import parameters\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# import Grid Search\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "import math\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "%run 'functions.ipynb'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_s3 = dg_s3.Connect('dgdatadump/DS&CA/Pricing/02 Projects/2022/Tel_Reg_2.0/Cancellation Model')\n",
    "\n",
    "df_raw12 = conn_s3.read('TelReg_Canx_filt_12mo.csv')\n",
    "df_raw9 = conn_s3.read('TelReg_Canx_filt_9mo.csv')\n",
    "df_raw6 = conn_s3.read('TelReg_Canx_filt_6mo.csv')\n",
    "df_raw3 = conn_s3.read('TelReg_Canx_filt_3mo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = ['df_raw3', 'df_raw6', 'df_raw9', 'df_raw12']\n",
    "data_set = [df_raw3, df_raw6, df_raw9, df_raw12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read feature importance performance pickle\n",
    "feat_imp_performance_df = pd.read_pickle(\"feat_imp_performance_df.pkl\")\n",
    "# read model performance pickle\n",
    "performance_df = pd.read_pickle(\"performance_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of df_raw3: 357058 rows\n",
      "size of df_raw6: 270538 rows\n",
      "size of df_raw9: 167819 rows\n",
      "size of df_raw12: 66895 rows\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_set)):\n",
    "    print(f'size of {data_list[i]}: {data_set[i].shape[0]} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: df_raw3\n",
      "predictors: ['ManufacturerBrandCode', 'ONS_%people_working_FT', 'ONS_avg_age', 'PlansAcceptedPast1YearCount', 'PlanLiveCount', 'ClientAccountDesc', 'PurchasePrice', 'AppAge', 'PlansActivePast5yrCount', 'ClientGroupDesc', 'ApplianceCode', 'ONS_n_families_per_population', 'Fee', 'price_diff', 'ONS_%people_working_49+', 'ONS_avg_distance_travelled_to_work(km)', 'ONS_avg_household_size', 'ONS_avg_dependent_children_per_family', 'ONS_bedrooms per rooms', 'GoodsColour']\n",
      "number of predictors: 20\n"
     ]
    }
   ],
   "source": [
    "# initialise variables for 3/6/9/12-month cancellation cohorts\n",
    "\n",
    "idx = 0\n",
    "predictors = performance_df.loc[idx, 'feats_stored_gbm']\n",
    "response = 'cancelflag'\n",
    "\n",
    "dataset = performance_df.loc[idx, 'name']\n",
    "print(f'dataset: {dataset}')\n",
    "print(f'predictors: {predictors}')\n",
    "print(f'number of predictors: {len(predictors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_set[idx].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ManufacturerBrandCode',\n",
       " 'ClientAccountDesc',\n",
       " 'ClientGroupDesc',\n",
       " 'ApplianceCode',\n",
       " 'GoodsColour']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_predictors = df[predictors].select_dtypes(include=['object']).columns.tolist()\n",
    "cat_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply leave one out encoder to non-ordinal feats\n",
    "for feature in ['ManufacturerBrandCode','ClientAccountDesc','ApplianceCode','ClientGroupDesc','GoodsColour']:\n",
    "    l1o_encoder, df[feature] = get_leave_one_out(df[feature], df['cancelflag'])\n",
    "    \n",
    "# # apply get_ordinal to ordinal feats\n",
    "# for feature in []:\n",
    "#     ord_encoder, df_[feature] = get_ordinal(df_[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predictors]\n",
    "y = df[response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    312570\n",
       "1     44488\n",
       "Name: cancelflag, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[idx][response].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled: Counter({0: 312570, 1: 156285})\n"
     ]
    }
   ],
   "source": [
    "# instantiating over and under sampler\n",
    "over = RandomOverSampler(sampling_strategy=0.5)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "\n",
    "# first performing oversampling to minority class\n",
    "X_over, y_over = over.fit_resample(X, y)\n",
    "print(f\"Oversampled: {Counter(y_over)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Random Sampling: Counter({0: 312570, 1: 156285})\n"
     ]
    }
   ],
   "source": [
    "# now to combine under sampling \n",
    "X_combined_sampling, y_combined_sampling = under.fit_resample(X_over, y_over)\n",
    "print(f\"Combined Random Sampling: {Counter(y_combined_sampling)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X_combined_sampling, y_combined_sampling], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTE(random_state=42)\n",
    "# X_res, y_res = sm.fit_resample(X, y)\n",
    "# print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n",
      "Warning: Your H2O cluster version is too old (5 months and 3 days)!Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>23 mins 44 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.0.4</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>5 months and 3 days !!!</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_ubuntu_89xuq0</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>29.88 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.9.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         23 mins 44 secs\n",
       "H2O_cluster_timezone:       Etc/UTC\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.0.4\n",
       "H2O_cluster_version_age:    5 months and 3 days !!!\n",
       "H2O_cluster_name:           H2O_from_python_ubuntu_89xuq0\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    29.88 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.9.7 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to factors\n",
    "for i in df.columns[0:]:\n",
    "    df[i] = df[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "gbm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "29.510124444961548\n",
      "gbm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.28      0.41     62675\n",
      "           1       0.37      0.85      0.52     31096\n",
      "\n",
      "    accuracy                           0.47     93771\n",
      "   macro avg       0.58      0.57      0.46     93771\n",
      "weighted avg       0.65      0.47      0.45     93771\n",
      "\n",
      "MCC: 0.17074 / 0.16770\n",
      "F1: 0.52001 / 0.51571\n",
      "AUC: 0.63185 / 0.62689\n",
      "AUC PR: 0.48111 / 0.47262\n",
      "Accuracy: 0.68287 / 0.68209\n",
      "Logloss: 0.60864 / 0.60881\n",
      "KS: 0.17972\n",
      "AUC of cross-validated holdout predictions: 0.6241710863153502\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# re train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[predictors], df[response], test_size = 0.2, random_state = 1)\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "test = pd.concat([x_test, y_test], axis=1)\n",
    "\n",
    "# preserve training & test data's index to be used as a key to merge prediction probability with main dataset \n",
    "xtrain_index = x_train.index\n",
    "xtest_index = x_test.index\n",
    "\n",
    "# convert to h2o dataframe format\n",
    "hf_train = h2o.H2OFrame(train)\n",
    "hf_test = h2o.H2OFrame(test)\n",
    "\n",
    "# format target to fit the model\n",
    "hf_train['cancelflag']=hf_train['cancelflag'].asfactor()\n",
    "hf_test['cancelflag']=hf_test['cancelflag'].asfactor()\n",
    "\n",
    "# initialise the estimator \n",
    "df_gbm = H2OGradientBoostingEstimator(keep_cross_validation_predictions=True, nfolds = 5, seed = 1)\n",
    "\n",
    "start = time.time()\n",
    "# train the model\n",
    "df_gbm.train(x = predictors, y = response, training_frame = hf_train)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "# classification report\n",
    "y_true = y_test\n",
    "y_pred = df_gbm.predict(hf_test)\n",
    "y_pred = y_pred.as_data_frame()\n",
    "\n",
    "print(classification_report(y_true, y_pred['predict'], labels=[0,1]))\n",
    "\n",
    "# performance metrics\n",
    "print_model_perf_stats(df_gbm, hf_train, hf_test)\n",
    "\n",
    "# AUC of cross-validated holdout predictions\n",
    "print(f'AUC of cross-validated holdout predictions: {df_gbm.auc(xval=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using H2O balance_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "# convert columns to factors\n",
    "for i in data_set[idx].columns[0:]:\n",
    "    df[i] = data_set[idx][i].astype('category')\n",
    "\n",
    "# re train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[predictors], df[response], test_size = 0.2, random_state = 1)\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "test = pd.concat([x_test, y_test], axis=1)\n",
    "\n",
    "# preserve training & test data's index to be used as a key to merge prediction probability with main dataset \n",
    "xtrain_index = x_train.index\n",
    "xtest_index = x_test.index\n",
    "\n",
    "# convert to h2o dataframe format\n",
    "hf_train = h2o.H2OFrame(train)\n",
    "hf_test = h2o.H2OFrame(test)\n",
    "\n",
    "# format target to fit the model\n",
    "hf_train['cancelflag']=hf_train['cancelflag'].asfactor()\n",
    "hf_test['cancelflag']=hf_test['cancelflag'].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with class_sampling_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using the class_sampling_factors parameter:\n",
    "# since all but Class 2 have similar frequency counts, let's undersample Class 2\n",
    "# and not change the sampling rate of the other classes\n",
    "# note: class_sampling_factors must be a list of floats\n",
    "sample_factors = [1., 0.23]\n",
    "balance_gbm = H2OGradientBoostingEstimator(balance_classes = True, class_sampling_factors = sample_factors, seed = 1234)\n",
    "\n",
    "start = time.time()\n",
    "balance_gbm.train(x = predictors, y = response, training_frame = hf_train, validation_frame = hf_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the values for `class_sampling_factors` to grid over\n",
    "\n",
    "hyper_params = {'class_sampling_factors': [[1., 1.867208334], [1., 0.23], [0.535558878, 1.]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this example uses cartesian grid search because the search space is small\n",
    "# and we want to see the performance of all models. For a larger search space use\n",
    "# random grid search instead: {'strategy': \"RandomDiscrete\"}\n",
    "# initialize the GBM estimator\n",
    "balance_gbm_2 = H2OGradientBoostingEstimator(balance_classes = True, seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build grid search with previously made GBM and hyperparameters\n",
    "grid = H2OGridSearch(model = balance_gbm_2, hyper_params = hyper_params,\n",
    "                     search_criteria = {'strategy': \"Cartesian\"})\n",
    "\n",
    "# train using the grid\n",
    "start = time.time()\n",
    "grid.train(x = predictors, y = response, training_frame = hf_train, validation_frame = hf_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the grid models by auc\n",
    "sorted_grid = grid.get_grid(sort_by='auc', decreasing=False)\n",
    "print(sorted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without class_sampling_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using the balance_classes parameter (set to True):\n",
    "balance_gbm = H2OGradientBoostingEstimator(balance_classes = True, seed = 1234)\n",
    "\n",
    "# train GBM model\n",
    "start = time.time()\n",
    "balance_gbm.train(x = predictors, y = response, training_frame = hf_train, validation_frame = hf_test)\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the values for `balance_classes` to grid over\n",
    "hyper_params = {'balance_classes': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this example uses cartesian grid search because the search space is small\n",
    "# and we want to see the performance of all models. For a larger search space use\n",
    "# random grid search instead: {'strategy': \"RandomDiscrete\"}\n",
    "# initialize the GBM estimator\n",
    "balance_gbm_2 = H2OGradientBoostingEstimator(seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build grid search with previously made GBM and hyperparameters\n",
    "grid = H2OGridSearch(model = balance_gbm_2, hyper_params = hyper_params,\n",
    "                     search_criteria = {'strategy': \"Cartesian\"})\n",
    "\n",
    "start = time.time()\n",
    "# train using the grid\n",
    "grid.train(x = predictors, y = response, training_frame = hf_train, validation_frame = hf_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the grid models by auc\n",
    "sorted_grid = grid.get_grid(sort_by='auc', decreasing=True)\n",
    "print(sorted_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a GBM model on the whole dataset (instead of the 60%) and using internal 5-fold cross-validation (re-using all other parameters including the seed)\n",
    "gbm = h2o.get_model(sorted_grid.sorted_metric_table()['model_ids'][0])\n",
    "\n",
    "#get the parameters from the Random grid search model and modify them slightly\n",
    "params = gbm.params\n",
    "new_params = {\"nfolds\":5, \"model_id\":None, \"training_frame\":None, \"validation_frame\":None, \n",
    "              \"response_column\":None, \"ignored_columns\":None}\n",
    "for key in new_params.keys():\n",
    "    params[key]['actual'] = new_params[key] \n",
    "gbm_best = H2OGradientBoostingEstimator()\n",
    "for key in params.keys():\n",
    "    if key in dir(gbm_best) and getattr(gbm_best,key) != params[key]['actual']:\n",
    "        setattr(gbm_best,key,params[key]['actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df = h2o.H2OFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format target to fit the model\n",
    "hf_df['cancelflag']=hf_df['cancelflag'].asfactor()\n",
    "\n",
    "# train GBM model on the whole dataset\n",
    "start = time.time()\n",
    "gbm_best.train(x=predictors, y=response, training_frame=hf_df)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "print(gbm_best.cross_validation_metrics_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "y_true = y_test\n",
    "y_pred = gbm_best.predict(hf_test)\n",
    "y_pred = y_pred.as_data_frame()\n",
    "\n",
    "print(classification_report(y_true, y_pred['predict'], labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.astype(int).values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.astype(int).values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_stats(gbm_best, hf_train, hf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'GBM_cancellation_model_12mo'\n",
    "# load the model\n",
    "tuned_gbm_best =  h2o.import_mojo(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_grid[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
